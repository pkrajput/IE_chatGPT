{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "OcNuN6k7YBUW"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import openai\n",
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(pdf_pathfile, maxlen = None):\n",
    "    reader = PdfReader(pdf_pathfile)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text +=page.extract_text()\n",
    "    if maxlen:\n",
    "        if len(text) > maxlen:\n",
    "            text = text[:maxlen]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info_from_pdf_chatgpt(url,savepath):\n",
    "    filename = url_to_filename(url, save_path, replace_signs=['https://www.','\\\\','/','+','*','?','=','%','#'])\n",
    "    if not os.path.exists(filename):\n",
    "        return np.nan\n",
    "    text = read_pdf(filename)\n",
    "    gpt_abstract = request_chatgpt(text)\n",
    "    return gpt_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info_chatgpt(text, questions, prev_dict):\n",
    "    message = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"The previous info dict is %s\" %prev_dict},\n",
    "            {\"role\": \"assistant\", \"content\": \"I have full understanding of this chunk of article: %s\" %text},\n",
    "            {\"role\": \"user\", \"content\": 'Extract these relevant information in English and organise it as \"Python dict\" (IMPORTANT), value set as \"n/a\" if not applicable and append if multiple answers come out. Be particularly cautious on the n/a in the previous info dict and try your best to update precise info. Questions: %s' %questions }\n",
    "        ]\n",
    "    response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\",messages=message,temperature=0)\n",
    "    extracted_info = response['choices'][0]['message']['content']\n",
    "    return response, extracted_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info_from_text(text, questions, thegap=6000):\n",
    "    steps = []\n",
    "    prev_dict = {}\n",
    "    responses = []\n",
    "    for i in range(int(len(text)/thegap) + 1):\n",
    "        paper_chunk = text[i*thegap: (i+1)*thegap]\n",
    "        response, extracted_info = extract_info_chatgpt(paper_chunk, questions, prev_dict)\n",
    "        responses.append({'i':i, 'response':response, 'chunk':paper_chunk, 'extracted_info':extracted_info})\n",
    "        try:\n",
    "            prev_dict = eval(extracted_info)\n",
    "        except:\n",
    "            prev_dict = extracted_info\n",
    "        steps.append(prev_dict)\n",
    "    final_memory = steps[-1]\n",
    "    return final_memory, steps, responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artiﬁcial Text Detection via Examining the Topology of Attention MapsLaida Kushnareva1\u0003,Daniil Cherniavskii2\u0003,Vladislav Mikhailov3\u0003,Ekaterina Artemova1,4,Serguei Barannikov2,5,Alexander Bernstein2,Irina Piontkovskaya1,Dmitri Piontkovski4,Evgeny Burnaev21Huawei Noah’s Ark lab,2Skolkovo Institute of Science and Technology,3SberDevices,4HSE University,5CNRS, IMJAbstractThe impressive capabilities of recent genera-tive models to create texts that are challeng-ing to distinguish from the human-written onescan be misused for generating fake news, prod-uct reviews, and even abusive content. Despitethe prominent performance of existing meth-ods for artiﬁcial text detection, they still lackinterpretability and robustness towards unseenmodels. To this end, we propose three noveltypes of interpretable topological features forthis task based on Topological Data Analysis(TDA) which is currently understudied in theﬁeld of NLP. We empirically show that thefeatures derived from the BERT model outper-form count- and neural-based baselines up to10% on three common datasets, and tend to bethe most robust towards unseen GPT-style gen-eration models as opposed to existing meth-ods. The probing analysis of the features re-veals their sensitivity to the surface and syn-tactic properties. The results demonstrate thatTDA is a promising line with respect to NLPtasks, speciﬁcally the ones that incorporate sur-face and structural information.1 IntroductionRecent text generation models (TGMs) based onthe transformer architecture (Vaswani et al., 2017)have demonstrated impressive capabilities of cre-ating texts which are very close to human in termsof ﬂuency, coherence, grammatical and factualcorrectness (Keskar et al., 2019; Zellers et al.,2020; Yang et al., 2019). Extensive GPT-styleTGMs (Radford et al., 2018) have achieved out-standing results over a great scope of NLP tasksemploying zero-shot, one-shot, and few-shot tech-niques, even outperforming state-of-the-art ﬁne-tuning approaches (Brown et al., 2020). However,such models can be misused for generating fakenews (Zellers et al., 2020; Uchendu et al., 2020),product reviews (Adelani et al., 2020), and even\u0003Equal contribution.extremist and abusive content (McGufﬁe and New-house, 2020).Many attempts have been made to developartiﬁcial text detectors (Jawahar et al., 2020),ranging from classical ML methods over count-based features (Uchendu et al., 2019) to advancedtransformer-based models (Adelani et al., 2020)and unsupervised approaches (Solaiman et al.,2019). Despite the prominent performance of thesemethods across various domains, they still lack in-terpretability and robustness towards unseen mod-els.This paper introduces a novel method for artiﬁ-cial text detection based on Topological Data Anal-ysis (TDA) which has been understudied in theﬁeld of NLP. The motivation behind this approachrelies on the fact that (i) the attention maps gener-ated by the transformer model can be representedas weighted bipartite graphs and thus can be efﬁ-ciently investigated with TDA, (ii) TDA methodsare known to capture well surface and structuralpatterns in data which, we believe, are crucial tothe task.The contributions are summarized as follows. (i)To the best of our knowledge, this work is the ﬁrstattempt to apply TDA methods over the transformermodel’s attention maps and interpret topologicalfeatures for the NLP ﬁeld. (ii) We propose threetypes of interpretable topological features derivedfrom the attention graphs for the task of artiﬁcialtext detection. We empirically show that a simplelinear classiﬁer trained on the TDA features pro-duced over BERT attentions (Devlin et al., 2019)outperforms count- and neural-based baselines upto 10%, and can perform on par with the fully ﬁne-tuned BERT model across three domains: socialmedia, news articles and product reviews. (iii) Test-ing the robustness towards unseen TGMs, we ﬁndthat the TDA-based classiﬁers tend to be more ro-bust as opposed to the existing detectors. (iv) Theprobing analysis of the features demonstrates theirarXiv:2109.04825v2 [cs.CL] 28 Apr 2022sensitivity to surface and syntactic properties. (v)Finally, we are publicly releasing the code1, hopingto facilitate the applicability of the TDA methodsto other NLP tasks, speciﬁcally the ones that incor-porate structural information.2 Related WorkApplications of Topological Data AnalysisTDA has been applied in NLP to study textual struc-tural properties, independent of their surface andsemantic peculiarities. These applications includedetection of children and adolescent writing (Zhu,2013), discourse and entailment in law documents(Savle et al., 2019), and exploring discourse proper-ties of the plot summary to identify the movie genre(Doshi and Zadrozny, 2018). Guan et al. (2016)apply the topologically motivated transformationof the document’s semantic graph to summarizeit further. However, these studies neither incor-porate neural data representations nor explore theproperties of neural language models.The research in the emerging scope of TDA ap-plications to neural networks and neural data repre-sentations has mainly focused on artiﬁcial datasetsor common problems in computer vision. The de-sired topological properties of the data representa-tion can be incorporated into the objective functionduring the training of a neural network, improvingits robustness and performance on the downstreamtasks such as human action recognition and imageclassiﬁcation (Som et al., 2020), image simpliﬁca-tion (Solomon et al., 2021), image segmentation(Clough et al., 2020) or generation (Gabrielssonet al., 2020). Another line aims to develop the topo-logical criteria of the network’s generalization prop-erties (Rieck et al., 2019; Corneanu et al., 2020;Naitzat et al., 2020; Barannikov et al., 2020) or itsrobustness to adversarial attacks (Corneanu et al.,2019).Exploring Attention Maps Several studies haveshown that attention maps of pre-trained languagemodels (LMs) capture linguistic information. Forthe sake of space, we will discuss only a few well-known recent works. Clark et al. (2019) attempt tocategorize the types of attention patterns observedin the BERT model. In particular, they discovercertain attention heads in which prepositions attendto their objects or coreferent mentions attend totheir antecedents. Further, they explore the typi-1https://github.com/danchern97/tda4atdcal behavior of the attention heads and introduceﬁve patterns, e.g. attending to the next token orprevious token, which the vast majority of the at-tention heads follow. Htut et al. (2019) explore thesyntactic information encoded in intra-word rela-tion in the attention maps. A maximum spanningtree (MST) is constructed from the computed at-tention weights and mapped to the correspondingdependency tree for a given sentence. This methodachieves a prominent Undirected Unlabeled Attach-ment Score (UUAS), indicating that the attentiongraphs indeed can capture the dependency-basedrelations. Michel et al. explore the importance ofthe attention heads with respect to a downstreamtask. They show that a large proportion of the at-tention heads can be pruned without harming themodel downstream performance. Beneﬁcially, thepruned model speeds up at the inference time. Fi-nally, visualization of the attention maps (Hooveret al., 2020) allows introspecting the model’s innerworkings interactively.Supervised Artiﬁcial Text Detectors Severalwell-established classical ML methods have beenapplied to the task of artiﬁcial text detection com-bined with topic modeling and linguistic features(Manjavacas et al., 2017; Uchendu et al., 2019,2020). The rise of pre-trained LMs has stimu-lated various improvements of the detectors. TheRoBERTa model (Liu et al., 2019) has demon-strated an outstanding performance with respectto many TGMs and domains (Adelani et al., 2020;Fagni et al., 2021). The capabilities of generativemodels such as GROVER (Zellers et al., 2020) andGPT-2 (Radford et al., 2019) have been also eval-uated on the task (Bahri et al., 2021). Last butnot least, Bakhtin et al. (2019) discriminate artiﬁ-cial texts by training a ranking energy-based modelover the outputs of a pre-trained LM.Unsupervised Artiﬁcial Text Detectors An-other line of methods incorporates probability-based measures combined with a set of pre-deﬁnedthresholds (Solaiman et al., 2019). Such methodsopen up a possibility of the human in the loopapproach where a human makes decisions withthe help of pre-trained LMs (Ippolito et al., 2020).The GLTR tool (Gehrmann et al., 2019) supportshuman-model interaction by visualizing the prop-erties of a text inferred by the model, which im-proves the human detection rate of artiﬁcial texts.A promising direction is involving acceptabilityand pseudo-perplexity metrics (Lau et al., 2020;Salazar et al., 2020) that can be used to evaluatetext plausibility.3 Background3.1 BERT ModelBERT is a transformer-based LM that has pushedstate-of-the-art results in many NLP tasks. TheBERT architecture comprises Lencoder layerswithHattention heads in each layer. The inputof each attention head is a matrix Xconsisting ofthed-dimensional representations (row-wise) of mtokens, so that Xis of shape m\u0002d. The headoutputs an updated representation matrix Xout:Xout=Wattn(XWV)withWattn= softmax\u0012(XWQ)(XWK)Tpd\u0013;(1)whereWQ,WK,WVare trained projection matri-ces of shape d\u0002dandWattnis of shapem\u0002mmatrix of attention weights. Each element wattnijcan be interpreted as a weight of the j-th input’srelation to thei-th output: larger weights meanstronger connection between the two tokens.3.2 Attention Map and Attention GraphAnattention map displays an attention matrixWattn(Equation 1) in form of a heat map, wherethe color of the cell (i;j)represents the relationof thei-th token to the output representation ofthej-th token. We use a graph representation ofthe attention matrix. The attention matrix is con-sidered to be a weighted graph with the verticesrepresenting tokens and the edges connecting pairsof tokens with strong enough mutual relation (thehigher the weight, the stronger the relation). Theconstruction of such graph appears to be quite prob-lematic: a threshold needs to be set to distinguishbetween weak and strong relations. This leads toinstability of the graph’s structure: changing thethreshold affects the graph properties such as thenumber of edges, connected components, cycles.The choice of the optimal thresholds is essentialto deﬁne which edges remain in the graph. TDAmethods allow extracting the overall graph’s prop-erties which describe the development of the graphwith respect to changes in the threshold.3.3 Topological Data AnalysisTDA instruments permit tracking the changes ofa topological structure across varying thresholdsfor different objects: scalar functions, point clouds,and weighted graph (Chazal and Michel, 2017).Given a set of tokens Vand an attention matrix ofpair-wise weights W, we build a family of graphstermed as ﬁltration : an ordered set of graphs forthe sequence of increasing thresholds. Figure 1depicts the ﬁltration for a toy example. First, webuild a graph for a small threshold, using which weﬁlter out the edges with the weights lower than thisthreshold. Next, we increase the threshold and con-struct the next graph. Then we compute the coretopological features of different dimensions: ford= 0these are connected components, for d= 1–“loops” (loosely speaking, they corresponds to basiccycles in a graph), and d-dimensional “holes” forhigher dimensions. The amounts of these featuresat each dimension \f",
      "0;\f",
      "1;:::;\f",
      " dare referred to asBetti numbers and serve as the main invariants ofthe objects in topology (see Appendix B for formaldeﬁnitions). While the threshold is increasing andthe edges are being ﬁltered, new features may arise.For example, the graph can decay into several con-nected components. At the same time, the featurescan also disappear when a cycle is broken. For eachfeature, we check the moment in the ﬁltration whenit appears (i.e., its “birth”) and when it disappears(i.e., its “death”). These moments are depicted ona diagram called barcode (see Figure 1). The bar-code’s horizontal axis corresponds to the sequenceof thresholds. Each horizontal line (“bar”) corre-sponds to a single feature (“hole”): the line lastsfrom the feature’s “birth” to the feature’s “death”.Barcodes characterize the “persistent” topologicalproperties of the graph, showing how stable topo-logical features are.We now detail building the attention graphs,the ﬁltration procedure, and the proposed featureswhich are derived from the attention graphs.4 Persistent Features of the AttentionGraphsInformal Deﬁnition and Interpretation We ex-tract three groups of features from the attentiongraphs. Topological features (Section 4.1) includea set of standard graph properties: the number ofconnected components, the number of edges, andthe number of cycles. These features are calcu-lated for each pre-deﬁned threshold separately and(a)[CLS]Iloveyou[SEP][CLS]Iloveyou[SEP](b)[CLS]Iloveyou[SEP](c)youI[CLS]love[SEP](d)youI[CLS]love[SEP](e)youI[CLS]love[SEP](f)youI[CLS]love[SEP](g) (h)Figure 1: Let us consider an attention map computed on the sentence “I love you” with the BERT model (Layer:1, Attention Head: 6) which is depicted in (a). After matching the vertices of the corresponding graph (b)andremoving directions as shown in (c)and(d), we get graph (e)with 5 vertices and 6 edges. For the sake of bettervisualization, we do not draw edges with a weight less than 0.2. The graph has one connected component ( \f",
      "0= 1)and two “loops” ( \f",
      "1= 2). After ﬁltering out edges with small weight, we get graph (f)which has one newconnected component (it is often referred to as “birth” of a new component) and does not have any “loops” (i.e.,the loops that we can see in the previous version of the graph have “died”). Consequently, after removing alledges, we get graph (g)where 3new connected components are born, and now there are 5connected components(\f",
      "0= 5) in total. The barcode (h)depicts 0-dimensional features (connected components) for the ﬁltration ((e),(f)and(g)). Here, the X-axis denotes the ﬁltration parameter \u000f, and the Y-axis denotes the number of the bars. Weignore the “inﬁnite” feature persisting through the whole ﬁltration. Note that conventionally on barcodes the xaxisis inverted.then concatenated. We consider two following vari-ants of the feature calculation: for a directed andan undirected attention graph. Barcode features(Section 4.2) are extracted from barcodes. Dis-tance to patterns (Section 4.3) is the group offeatures derived from the attention maps by com-puting the distance to the attention patterns (Clarket al., 2019).To give the linguistic interpretation of our fea-tures, recall that the graph structures are used inlexicology for describing semantic change laws(Hamilton et al., 2016; Lipka, 1990; Arnold, 1973).The evolution of the meaning of a word with timecan be represented as a graph, in which edges rep-resent a semantic shift to different word meanings.Two typical patterns are distinguished in the graphstructure: radiation – the “star” structure, wherethe primary meaning is connected to other conno-tations independently; concatenation , orchainingshift – the “chain” structure when the connotationsare integrated one-by-one. Note that the typical at-tention patterns (Clark et al., 2019) have the same“radiation” and “concatenation” structure. In pre-trained LMs, the evolution goes through the layersof the model, changing the representation of eachtoken, ending up with highly contextualized tokenrepresentations, and the aggregated representationof the whole sentence (in the form of the [CLS]-token).We consider persistent features as the numer-ical characteristic of the semantic evolution pro-cesses in the attention heads. Topological featuresdeal with clusters of mutual inﬂuence of the to-kens in the sentence and the local structures likechains and cycles. The barcode features charac-terize the severity and robustness of the semanticchanges. The features with long persistence (largedistance between “birth” and “death”) correspondto the stable processes which dominate the others,while short segments in the barcode deﬁne pro-cesses highly inﬂuenced by noise. Pattern featuresprovide a straightforward measure of the presenceof typical processes over the whole sentence. Theso-called “vertical” pattern corresponds to the “ra-diation” around the single token when the meaningof the sentence or a part of the sentence is aggre-gated from all words equally. “Diagonal” patternrepresents consequent “concatenation” structure,going through all the sentence and thus reﬂectingthe dependence of each token’s meaning on its leftcontext.4.1 Topological FeaturesFirst, we ﬁx a set of thresholds T=ftigki=1;0<t1< ::: < t k<1. Consider an attention headhand corresponding weights Wattn= (wattni;j).Given a text sample s, for each threshold levelt2Twe deﬁne the weighted directed graph \u0000hs(t)with edgesfj!ijwattnij\u0015tgand its undirectedvariant \u0000hs(t)by setting an undirected edge vivjforeach pair of vertices viandvjwhich are connectedby an edge in at least one direction in the graph\u0000hs(t).We consider the following features of the graphs:•the ﬁrst two Betti numbers of the undirectedgraph \u0000hs(t). The feature calculation proce-dure is described in Appendix A;•the number of edges ( e), the number ofstrongly connected components ( s) and theamount of simple directed cycles ( c) in thedirected graph \u0000hs(t).To get the whole set of topological features forthe given text sample sand the attention head h,we concatenate the features for all the thresholds,starting from T.4.2 Features Derived from BarcodesFor each text sample we calculate barcodes of theﬁrst two persistent homology groups (denoted asH0andH1) on each attention head of the BERTmodel (see Appendix B for further details). Wecompute the following characteristics of these bar-codes:• The sum of lengths of bars;• The mean of lengths of bars;• The variance of lengths of bars;•The number of bars with time of birth/deathgreater/lower than threshold;•The time of birth/death of the longest bar (ex-cluding inﬁnite);• The overall number of bars;• The entropy of the barcode.4.3 Features Based on Distance to PatternsThe shape of attention graphs in distinct attentionheads can be divided into several patterns (Clarket al., 2019). We hypothesize that appearance ofsuch patterns in a particular head or “intensity” ofthe pattern (i.e., the threshold ton which the patternappears) may carry essential linguistic information.Thus, we formalize these attention patterns andcalculate the distances to them as follows.LetA= (aij)be an incidence matrix of theText Source Train Validation Test jVocabj LengthH M H M H M H M H MWebTextGPT-2 Small;pure sampling20K 20K 2.5K 2.5K 2.5K 2.5K 220K 532K 593 \u0006177 515 \u0006322AmazonReviewGPT-2 XLpure sampling5K 5K 1K 1K 4K 4K 47K 49K 179 \u0006170 177 \u0006171RealNewsGROVERtop-psampling5K 5K 1K 1K 4K 4K 98K 75K 721 \u0006636 519 \u0006203Table 1: Statistics for the datasets used in the experiments on the artiﬁcial text detection task. H=Human;M=Machine.graph \u0000withnvertices, where aij= 1for all edges(ij)2Eand0for all other i;j. Let \u0000 = (V;E)and\u00000= (V;E0)be two graphs with the sameset of vertices, and let A,A0be their incidencematrices. As a distance dbetween such graphs weuse Frobenius norm of the difference jjA\u0000A0jjF=qPi;j(aij\u0000a0ij)2;normalized by the norms ofthe matrices of compared graphs:d(\u0000;\u00000) =jjA\u0000A0jjFqjjAjj2F+jjA0jj2F=vuutPi;j(aij\u0000a0ij)2Pi;j(a2ij+a0ij2):Such distance takes values between 0 and 1. Forthe unweighted graphs we have:d(\u0000;\u00000) =sjE4E0jjEj+jE0j;whereE4E0= (EnE0)S(E0nE)is the symmet-ric difference of sets EandE0.We consider distances from the given graph \u0000toattention patterns \u0000ias the graph features di(\u0000) =d(\u0000;\u0000i), and the patterns posed by (Clark et al.,2019):•Attention to the previous token. \u0000feature :E= (i+ 1;i),i=1;n\u00001.•Attention to the next token. \u0000feature :E=(i;i+ 1) ,i=1;n\u00001.•Attention to [CLS]-token. [CLS]-token corre-sponds to the vertex 1 of the set V= [1;n]asit denotes the beginning of the text. \u0000feature :E= (i;1),i=1;n.• Attention to [SEP]-token. Suppose i1;:::;i kare the indices of [SEP]-tokens. Then\u0000feature :E= (i;it),i=1;n,t=1;k.•Attention to punctuation marks. Let i1;:::;i kbe the indices of the tokens which correspondto commas and periods. \u0000feature :E=(i;it),i=1;n,t=1;k. Note that this pat-tern can be potentially divided into Attentionto commas and Attention to periods.5 Experiments5.1 Artiﬁcial Text DetectionData We prepare three datasets from differentdomains to conduct the experiments on the taskof artiﬁcial text detection. Table 1 outlines statis-tics for the datasets. Each split is balanced by thenumber of samples2per each target class.WebText & GPT-2 comprises a subset of naturaland generated texts from the datasets proposed byRadford et al. (2018). (i) WebText contains ﬁlteredand de-duplicated natural texts from Reddit; (ii)GPT-2 Output Dataset3includes texts generatedby various versions of the GPT-2 model ﬁne-tunedonWebText . We use texts generated by GPT-2Small (117M) with pure sampling.Amazon Reviews & GPT-2 consists of a subset ofAmazon product reviews (Amazon, 2019) and textsgenerated by GPT-2 XL (1542M) with pure sam-pling, ﬁne-tuned on this dataset (Solaiman et al.,2019).RealNews & GROVER (Zellers et al., 2020) in-cludes a subset of the news articles from RealNews(that are not present in the GROVER training data)and news articles generated by GROVER with top-psampling.2Each sample is truncated to 128 BertTokenizer tokens(bert-base-uncased ).3https://github.com/openai/gpt-2-output-datasetModelWebText &GPT-2 SmallAmazon Reviews &GPT-2 XLRealNews &GROVERTF-IDF, N-grams 68.1 54.2 56.9BERT [CLS trained] 77.4 54.4 53.8BERT [Fully trained] 88.7 60.1 62.9BERT [SLOR] 78.8 59.3 53.0Topological features 86.9 59.6 63.0Barcode features 84.2 60.3 61.5Distance to patterns 85.4 61.0 62.3All features 87.7 61.1 63.6Table 2: The results of the artiﬁcial text detection experiments. The performance is measured by the accuracyscore (%).Baselines We use bert-base-uncased4model from the HuggingFace library (Wolf et al.,2020) for the BERT-based baselines described be-low. (i) BERT [CLS trained] is a linear layertrained over [CLS]-pooled text representations.Note that the weights of the BERT model re-main frozen. (ii) BERT [Fully trained] is afully ﬁne-tuned BERT model. We also train Lo-gistic Regression classiﬁer from scikit-learn li-brary (Pedregosa et al., 2011) over (iii) TF-IDF,N-grams with the N-gram range 2[1;2]and(iv)BERT [SLOR] (Pauls and Klein, 2012), anpseudo-perplexity-based acceptability measure in-ferred with the BERT model under the implemen-tation by Lau et al. (2020)5.Models We train Logistic Regression classiﬁerover the persistent graph features derived from theattention matrices from the BERT model: (i) Topo-logical features (Section 4.1), (ii) Barcode fea-tures (Section 4.2) and (iii) Distance to patterns(Section 4.3). (iv) All features is the concatena-tion of the features mentioned above. The trainingdetails for the baselines and models are outlined inAppendix C.Results Table 2 outlines the results of the artiﬁ-cial text detection experiments on the three datasets.Note the diversity of the experiment setting wherethe methods are tested with respect to the TGM,TGM’s size, the decoding method, domain, andstylistic properties (texts from the Amazon Re-views & GPT-2 are shorter as compared to those ofWebText & GPT-2 andRealNews & GROVER ).The overall tendency is that the proposed TDA-4https://huggingface.co/bert-base-uncased5https://github.com/jhlau/acceptability-prediction-in-contextbased classiﬁers outperform the count-based ( TF-IDF, N-grams ) and two BERT-based baselines(BERT [CLS trained] ,BERT [SLOR] ) up to10%. The concatenation of the features achievesthe performance on par with the fully trained BERTmodel on all datasets.5.2 Robustness towards Unseen ModelsThis setting tests the robustness of the artiﬁcialtext detection methods towards unseen TGMs ontheWebText & GPT-2 dataset. The baselines andmodels are trained on texts from the GPT-2 smallmodel and further used to detect texts generatedby unseen GPT-style models with pure sampling:GPT-2 Medium (345M), GPT-2 Large (762M) andGPT-2 XL (1542M). Note that such a setting is themost challenging as it requires the transfer fromthe smallest model to that of the higher number ofparameters (Jawahar et al., 2020).Results Figure 2 demonstrates the results on therobustness experimental setup. The simple lin-ear classiﬁer trained over the Topological featuresdemonstrates the minor performance drop on thetask of detecting artiﬁcial texts by the larger GPT-style models as opposed to the considered meth-ods. However, the TDA-based classiﬁer performsslightly worse than BERT [Fully trained] on thetest subset by GPT-2 Small.5.3 Attention Head-wise ProbingData SentEval (Conneau et al., 2018) is a com-mon probing suite for exploring how various lin-guistic properties are encoded in the model rep-resentations. The probing tasks are organized bythe type of the property: surface ,syntactic andsemantic . We use the undersampled tasks6to ana-6Each probing task is split into 25K/5K/5Ktrain/validation/test sets. The sets are balanced by theFigure 2: The results of the robustness experiments. X-axis=GPT-2 model size. Y-axis=Accuracy score.lyze what properties are stored in the topologicalfeatures.Method Attention head-wise probing (Jo andMyaeng, 2020) allows investigating the patterns ofhow attention heads from each layer of the modelcontribute most to a probing task. Logistic Re-gression is trained over the intermediate outputs ofthe modelhi;j, where iandjdenote the indices ofthe layer and the attention head. We use the pub-licly available code7to train the classiﬁer over twogroups of the input features: (i) the intermediateoutputshi;jproduced by the frozen BERT modeland (ii) the topological features derived from hi;jas outlined in Sections 4.1, 4.2. The performanceis evaluated by the accuracy score, and the heatmaps of the probing scores are constructed to intro-spect how a certain linguistic property is distributedacross different layers and attention heads. Referto Jo and Myaeng (2020) for more details.Results The results demonstrate that the topo-logical features tend to be sensitive to the surfaceand syntactic properties as opposed to the seman-tic ones. Figure 3 shows heat maps of the atten-tion head-wise evaluation on LENGTH (Figure 3a,surface property) and DEPTH (Figure 3b, syntac-ticproperty) tasks8. While the sentence lengthis distributed across the majority of the frozen at-tention heads, speciﬁcally at the lower-to-middlelayers [1\u00008], the topological features capture theproperty at layer [1]and by fewer heads at lay-ers[2;4\u00005;9\u000011]. The depth of the syntaxnumber of instances per each target class.7https://github.com/heartcored98/transformer_anatomy8LENGTH is a 6-way classiﬁcation task and DEPTH com-prises 7 classes denoting the depth of a syntax tree.tree is encoded in the frozen heads at the lower-to-middle layers [1\u00005], whereas the barcode featurespredominantly localize the property at the middle-to-higher layers [5\u00009].The overall pattern for the surface and syntac-tic tasks is that the persistent graph features canlosesome information on the linguistic propertiesduring the derivation of the features from the at-tention matrices. The localization of the propertiesafter the derivation gets changed, and the head-wiseprobe performance may signiﬁcantly decrease. No-tably, the majority of the semantic tasks receiverapid decreases in the probe performance on thepersistent graph features as compared to the frozenheads. The reason is that the features operate purelyon the surface and structural information of the at-tention graph, leaving semantics unattended.6 DiscussionStructural Differences between Natural andGenerated texts The TDA-based classiﬁers relyon the structural differences in the topology of theattention maps to distinguish between natural andgenerated texts. Figure 4 shows that the distribu-tions of the sum of bars in H0differ for naturaland generated texts. For the former, it is shifted tothe left. We provide more examples of the distri-bution shift for different heads and layers in Fig-ure 5, Appendix B. The weights for natural texts areconcentrated more on the edges of the maximumspanning tree (MST), so that the model focuseson the sentence structure, or on the “skeleton” ofthe MST. The weights for the artiﬁcially generatedtexts are distributed more evenly among all edges.As the TDA-based classiﬁers appear to be robusttowards unseen TGMs, we may conclude that suchstructural properties are inherent to the models ofdifferent sizes, so that shifts in the distribution ofthe sum of bars in H0hold for texts generated bydifferent TGMs. This feature appears to be the keyone as utilizing it alone for the prediction providesus with the 82% accuracy score on the WebText &GPT-2 dataset.Semantics is Limited The TDA-based methodsdo not take the semantic word similarity into ac-count, as they only capture inter-word relations de-rived from the attention graphs. The probing analy-sis supports the fact that the features do not encodethe semantic properties, carrying only surface andstructural information. However, this informationappears to be sufﬁcient for the considered task.(a) L ENGTH (b) D EPTHFigure 3: Heat maps of attention head-wise probing on L ENGTH (Left) and D EPTH (Right) tasks. Atten-tions=Frozen attention weights. X-axis=Head index number. Y-axis=Layer index number. The brighter the color,the higher the accuracy score for the attention head.Figure 4: The distribution shift of the sum of the barsinH0between the natural and generated texts on theWebText & GPT-2 dataset (Layer: 9; Head: 7). TGM:GPT-2 Small with pure sampling.Time Complexity The attention matrices arecomputed each time when an input sample is fed toBERT. It follows that the computational complexityof our methods can not be lower than the one forBERT’s complexity itself, which makes asymp-toticallyO(n2d+nd2)per one attention head(Vaswani et al., 2017), where nis the sequencelength, anddis the words embedding dimension.On the other hand, the calculation of the topologi-cal features by thresholds (given that the number ofthresholds is constant), aside of the number of sim-ple cycles, features of 0-dimensional barcodes, andfeatures based on the distance to patterns are lin-ear by the number of edges of the attention graphs.This means that for at least these features we do notgo beyond the asymptotic complexity of the BERTmodel inference, even for sparse attention variants.The number of simple cycles and the featuresof1-dimensional barcodes are more computation-ally expensive. Note that omitting these featuresprovides a signiﬁcant speed up with minor perfor-mance drops.7 ConclusionThis paper introduces a novel method for the taskof artiﬁcial text detection based on TDA. We pro-pose three types of interpretable topological fea-tures that can be derived from the attention mapsof any transformer-based LM. The experimentsdemonstrate that simple linear classiﬁers trainedon these features can outperform count- and neural-based baselines, and perform on par with a fullyﬁne-tuned BERT model on three common datasetsacross various domains. The experimental setupalso highlights the applicability of the features to-wards the TGM architecture, TGM’s size and thedecoding method. Notably, the TDA-based classi-ﬁers tend to be more robust towards unseen GPT-style TGMs as opposed to the considered baselinedetectors. The probing analysis shows that thefeatures capture surface and structural properties,lacking the semantic information. A fruitful direc-tion for future work is to combine the topologicalfeatures with those that encode the semantics of theinput texts, and test the methods on a more diverseset of the TGM architectures, decoding methodsand transformer LMs to infer the attention graphsfrom. We are publicly releasing the code, hoping tostimulate the research on the TDA-based investiga-tion of the inner workings of the transformer-basedmodels and the applicability of TDA methods toother NLP tasks.AcknowledgementThe work of Serguei Barannikov and Evgeny Bur-naev, related to topological data analysis and ma-chine learning in Sections 4 and 5, is supported byMinistry of Science and Higher Education grantNo. 075-10-2021-068. Ekaterina Artemova is sup-ported by the framework of the HSE UniversityBasic Research Program.ReferencesDavid Ifeoluwa Adelani, Haotian Mai, Fuming Fang,Huy H Nguyen, Junichi Yamagishi, and IsaoEchizen. 2020. Generating sentiment-preservingfake online reviews using neural language modelsand their human-and machine-based detection. InInternational Conference on Advanced InformationNetworking and Applications , pages 1341–1354.Springer.Amazon. 2019. Amazon Customer ReviewsDataset. https://s3.amazonaws.com/amazon-reviews-pds/readme.html .I.V . Arnold. 1973. The English Word.Dara Bahri, Yi Tay, Che Zheng, Cliff Brunk, DonaldMetzler, and Andrew Tomkins. 2021. Generativemodels are unsupervised predictors of page quality:A colossal-scale study. In Proceedings of the 14thACM International Conference on Web Search andData Mining , pages 301–309.Anton Bakhtin, Sam Gross, Myle Ott, Yuntian Deng,Marc’Aurelio Ranzato, and Arthur Szlam. 2019.Real or fake? learning to discriminate machine fromhuman generated text.Serguei Barannikov. 1994. Framed Morse complexesand its invariants. Adv. Soviet Math. , 21:93–115.Serguei Barannikov. 2021. Canonical Forms = Persis-tence Diagrams. Tutorial. In European Workshop onComputational Geometry (EuroCG 2021) .Serguei Barannikov, Grigorii Sotnikov, Ilya Troﬁmov,Alexander Korotin, and Evgeny Burnaev. 2020.Topological Obstructions in Neural Networks Learn-ing. arXiv preprint arXiv:2012.15834 .Ulrich Bauer. 2021. Ripser: Efﬁcient Computation ofVietoris–Rips Persistence Barcodes. Journal of Ap-plied and Computational Topology , pages 1–33.Tom B. Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-V oss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,Clemens Winter, Christopher Hesse, Mark Chen,Eric Sigler, Mateusz Litwin, Scott Gray, BenjaminChess, Jack Clark, Christopher Berner, Sam Mc-Candlish, Alec Radford, Ilya Sutskever, and DarioAmodei. 2020. Language Models are Few-ShotLearners.Frédéric Chazal and Bertrand Michel. 2017. An Intro-duction to Topological Data Analysis: Fundamen-tal and Practical Aspects for Data Scientists. arXivpreprint arXiv:1710.04019 .Kevin Clark, Urvashi Khandelwal, Omer Levy, andChristopher D Manning. 2019. What does bert lookat? an analysis of bert’s attention. In Proceedings ofthe 2019 ACL Workshop BlackboxNLP: Analyzingand Interpreting Neural Networks for NLP , pages276–286.James Clough, Nicholas Byrne, Ilkay Oksuz,Veronika A Zimmer, Julia A Schnabel, andAndrew King. 2020. A Topological Loss Functionfor Deep Learning-based Image SegmentationUsing Persistent Homology. IEEE Transactions onPattern Analysis and Machine Intelligence .Alexis Conneau, Germán Kruszewski, Guillaume Lam-ple, Loïc Barrault, and Marco Baroni. 2018. WhatYou Can Cram into a Single Vector: Probing Sen-tence Embeddings for Linguistic Properties. In Pro-ceedings of the 56th Annual Meeting of the Associa-tion for Computational Linguistics (Volume 1: LongPapers) , pages 2126–2136.Ciprian A Corneanu, Sergio Escalera, and Aleix MMartinez. 2020. Computing the Testing Error with-out a Testing Set. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recog-nition , pages 2677–2685.Ciprian A Corneanu, Meysam Madadi, Sergio Escalera,and Aleix M Martinez. 2019. What Does it Meanto Learn in Deep Networks? And, how Does OneDetect Adversarial Attacks? In Proceedings of theIEEE/CVF Conference on Computer Vision and Pat-tern Recognition , pages 4757–4766.Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2019. BERT: Pre-training ofDeep Bidirectional Transformers for Language Un-derstanding. In NAACL-HLT (1) .Pratik Doshi and Wlodek Zadrozny. 2018. MovieGenre Detection Using Topological Data Analysis.InInternational Conference on Statistical Languageand Speech Processing . Springer, Cham.Tiziano Fagni, Fabrizio Falchi, Margherita Gambini,Antonio Martella, and Maurizio Tesconi. 2021.TweepFake: About Detecting Deepfake Tweets.Plos one , 16(5):e0251415.Rickard Brüel Gabrielsson, Bradley J Nelson, AnjanDwaraknath, and Primoz Skraba. 2020. A Topol-ogy Layer for Machine Learning. In InternationalConference on Artiﬁcial Intelligence and Statistics ,pages 1553–1563. PMLR.Sebastian Gehrmann, Hendrik Strobelt, and AlexanderRush. 2019. GLTR: Statistical detection and visu-alization of generated text. In Proceedings of the57th Annual Meeting of the Association for Compu-tational Linguistics: System Demonstrations , pages111–116, Florence, Italy. Association for Computa-tional Linguistics.Hui Guan, Wen Tang, Hamid Krim, James Keisert, An-drew Rindos, and Radmila Sazdanovic. 2016. ATopological Collapse for Document Summarization.InIEEE 17th International Workshop on SignalProcessing Advances in Wireless Communications(SPAWC) .William L Hamilton, Jure Leskovec, and Dan Jurafsky.2016. Diachronic word embeddings reveal statisti-cal laws of semantic change. In Proceedings of the54th Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers) , pages1489–1501.Benjamin Hoover, Hendrik Strobelt, and SebastianGehrmann. 2020. exBERT: A visual analysis tool toexplore learned representations in Transformer mod-els. In Proceedings of the 58th Annual Meeting ofthe Association for Computational Linguistics: Sys-tem Demonstrations , pages 187–196, Online. Asso-ciation for Computational Linguistics.Phu Mon Htut, Jason Phang, Shikha Bordia, andSamuel R Bowman. 2019. Do attention Headsin BERT Track Syntactic Dependencies? arXivpreprint arXiv:1911.12246 .Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, and Douglas Eck. 2020. Automatic detec-tion of generated text is easiest when humans arefooled. In Proceedings of the 58th Annual Meet-ing of the Association for Computational Linguistics ,pages 1808–1822, Online. Association for Computa-tional Linguistics.Ganesh Jawahar, Muhammad Abdul-Mageed, andVS Laks Lakshmanan. 2020. Automatic detectionof machine generated text: A critical survey. InProceedings of the 28th International Conference onComputational Linguistics , pages 2296–2309.Jae-young Jo and Sung-Hyon Myaeng. 2020. Rolesand Utilization of Attention Heads in Transformer-based Neural Language Models. In Proceedings ofthe 58th Annual Meeting of the Association for Com-putational Linguistics , pages 3404–3417.Nitish Shirish Keskar, Bryan McCann, Lav RVarshney, Caiming Xiong, and Richard Socher.2019. CTRL: A conditional transformer languagemodel for controllable generation. arXiv preprintarXiv:1909.05858 .Jey Han Lau, Carlos Armendariz, Shalom Lappin,Matthew Purver, and Chang Shu. 2020. How furi-ously can colorless green ideas sleep? sentence ac-ceptability in context. Transactions of the Associa-tion for Computational Linguistics , 8:296–310.Leonhard Lipka. 1990. An Outline of English Lexicol-ogy. de Gruyter.Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,Luke Zettlemoyer, and Veselin Stoyanov. 2019.Roberta: A robustly optimized bert pretraining ap-proach.Enrique Manjavacas, Jeroen De Gussem, Walter Daele-mans, and Mike Kestemont. 2017. Assessing thestylistic properties of neurally generated text in au-thorship attribution. In Proceedings of the Workshopon Stylistic Variation , pages 116–125.Kris McGufﬁe and Alex Newhouse. 2020. TheRadicalization Risks of GPT-3 and AdvancedNeural Language Models. arXiv preprintarXiv:2009.06807 .Paul Michel, Omer Levy, and Graham Neubig. are six-teen heads really better than one? In Advances inNeural Information Processing Systems . Curran As-sociates, Inc.Gregory Naitzat, Andrey Zhitnikov, and Lek-HengLim. 2020. Topology of Deep Neural Networks.Journal of Machine Learning Research , 21(184):1–40.Adam Pauls and Dan Klein. 2012. Large-scale syntac-tic language modeling with treelets. In Proceedingsof the 50th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers) ,pages 959–968.Fabian Pedregosa, Gaël Varoquaux, Alexandre Gram-fort, Vincent Michel, Bertrand Thirion, OlivierGrisel, Mathieu Blondel, Peter Prettenhofer, RonWeiss, Vincent Dubourg, et al. 2011. Scikit-learn:Machine learning in Python. the Journal of machineLearning research , 12:2825–2830.Alec Radford, Jeffrey Wu, Rewon Child, David Luan,Dario Amodei, and Ilya Sutskever. 2018. Languagemodels are unsupervised multitask learners.Alec Radford, Jeffrey Wu, Rewon Child, David Luan,Dario Amodei, and Ilya Sutskever. 2019. Languagemodels are unsupervised multitask learners. OpenAIblog, 1(8):9.Bastian Rieck, Matteo Togninalli, Christian Bock,Michael Moor, Max Horn, Thomas Gumbsch, andKarsten Borgwardt. 2019. Neural Persistence: AComplexity Measure for Deep Neural Networks Us-ing Algebraic Topology. In International Confer-ence on Learning Representations (ICLR) .Julian Salazar, Davis Liang, Toan Q. Nguyen, and Ka-trin Kirchhoff. 2020. Masked language model scor-ing. In Proceedings of the 58th Annual Meetingof the Association for Computational Linguistics ,pages 2699–2712, Online. Association for Compu-tational Linguistics.Ketki Savle, Wlodek Zadrozny, and Minwoo Lee. 2019.Topological data analysis for discourse semantics?InProceedings of the 13th International Conferenceon Computational Semantics - Student Papers , pages34–43, Gothenburg, Sweden. Association for Com-putational Linguistics.Irene Solaiman, Miles Brundage, Jack Clark, AmandaAskell, Ariel Herbert-V oss, Jeff Wu, Alec Radford,Gretchen Krueger, Jong Wook Kim, Sarah Kreps,Miles McCain, Alex Newhouse, Jason Blazakis,Kris McGufﬁe, and Jasmine Wang. 2019. ReleaseStrategies and the Social Impacts of Language Mod-els.Yitzchak Solomon, Alexander Wagner, and Paul Ben-dich. 2021. A fast and robust method for global topo-logical functional optimization. In InternationalConference on Artiﬁcial Intelligence and Statistics ,pages 109–117. PMLR.Anirudh Som, Hongjun Choi, Karthikeyan Natesan Ra-mamurthy, and Matthew P. Buman. 2020. Pi-net: ADeep Learning Approach to Extract Topological Per-sistence Images. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recog-nition Workshops .Adaku Uchendu, Jeffery Cao, Qiaozhi Wang, Bo Luo,and Dongwon Lee. 2019. Characterizing man-madevs. machine-made chatbot dialogs. In Proceedingsof the Int’l Conf. on Truth and Trust Online (TTO) .Adaku Uchendu, Thai Le, Kai Shu, and Dongwon Lee.2020. Authorship attribution for neural text gener-ation. In Proceedings of the 2020 Conference onEmpirical Methods in Natural Language Process-ing (EMNLP) , pages 8384–8395, Online. Associa-tion for Computational Linguistics.Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N. Gomez, LukaszKaiser, and Illia Polosukhin. 2017. Attention Is AllYou Need. In Advances in neural information pro-cessing systems , pages 5998–6008.Thomas Wolf, Julien Chaumond, Lysandre Debut, Vic-tor Sanh, Clement Delangue, Anthony Moi, Pier-ric Cistac, Morgan Funtowicz, Joe Davison, SamShleifer, et al. 2020. Transformers: State-of-the-art Natural Language Processing. In Proceedings ofthe 2020 Conference on Empirical Methods in Nat-ural Language Processing: System Demonstrations ,pages 38–45.Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-bonell, Russ R Salakhutdinov, and Quoc V Le. 2019.Xlnet: Generalized Autoregressive Pre-training forLanguage Understanding. Advances in neural infor-mation processing systems , 32.Rowan Zellers, Ari Holtzman, Hannah Rashkin,Yonatan Bisk, Ali Farhadi, Franziska Roesner, andYejin Choi. 2020. Defending Against Neural FakeNews. Neurips .Simon Zhang and Mengbai Xiao. 2020. GPU-accelerated computation of Vietoris-Rips persis-tence barcodes. In Proceedings of Symposium ofComputational Geometry (SoCG 2020) .Xiaojin Zhu. 2013. Persistent Homology: An Intro-duction and a New Text Representation for NaturalLanguage Processing. In IJCAI .AppendicesA Topological Features CalculationAlgorithm 1 Topological Features CalculationRequire: Text sample sRequire: Set of chosen attention heads HMofmodelMRequire: Thresholds array TRequire: Topological feature fof unweightedgraphEnsure: Features array Featuresprocedure FEATURES _CALC (s;HM;T)2: for allh2HM;t2TdoCalculate attention graph \u0000hs=(V;E;Watth;s)on sampleson headh4:Ehs(t) fe2E(\u0000hs) :Watth;s(e)\u0015tg\u0000hs(t) (V;Ehs(t))6:Ehs(t) \bfi;jg: (i;j)2Ehs(t)\t\u0000hs(t) (V;Ehs(t))8: Calculatef(\u0000hs(t))end for10:Features hf(\u0000hs(t))ih2HMt2TreturnFeatures12:end procedureB Persistent Homology and BettiNumbersRecall that a simplicial complex Kis a collectionof subsets of a ﬁnite set called simplices such thateach subset of any element of Kalso is an elementofK; such subsets of a simplex are called faces . Inparticular, an undirected graph is a simplicial com-plex where the simplices correspond to the edgesand vertices of the graph. The set of all formalZ-linear combinations of the p-dimensional sim-plices (that is, (p+ 1) -element ﬁnite sets from thecollection) of Kis denoted Cp(K). These linearcombinations c=Pj\r",
      "j\u001bjare calledp-chains ,where the\r",
      "j2Zand the\u001bjarep-simplices in K.The boundary, @(\u001bj), is the formal sum of the(p\u00001)-dimensional faces of \u001bjand the boundaryof the chain is obtained by extending @linearly,@(c) =Xj\r",
      "j@(\u001bj)forcas above.Thep-chains that have boundary 0are calledp-cycles , they form a subgroup Zp(K)ofCp(K).Thep-chains that are the boundary of (p+ 1) -chains are called p-boundaries and form a subgroupBp(K)ofCp(K). The quotient group Hp(K) =Cp(K)=Bp(K)is called the p-thhomology ofK.Their ranks \f",
      "p=rk(Hp(K))of these abeliangroups are called Betti numbers . The homologyand the Betti numbers are classical topological in-variants ofK.In particular, a graph G= (E;V)contains 0-dimensional and 1-dimensional faces. It followsthat its topological form is essentially describedby the numbers \f",
      "0and\f",
      "1, which are the onlynonzero Betti numbers. Here \f",
      "0is the numberof connected components of G, and\f",
      "1is the num-ber of independent cycles of the graph (which isequal tojEj\u0000jVj+\f",
      "0).Asubcomplex ofKis a subset of simplices thatis closed under the face relation. A ﬁltration ofKis a nested sequence of subcomplexes that startswith the empty complex and ends with the completecomplex,;\u001aKt1\u001aKt2\u001a\u0001\u0001\u0001\u001aKtm=K:In particular, to any weighted undirected graphG= (V;E)one can associate naturally the ﬁltra-tion;\u001aGt1\u001a\u0001\u0001\u0001\u001aGtm=G; (2)whereftigis the set of all weights of the edges,Gti= (V;E ti)andEticonsists of all edges of Ewith weight less or equal to ti.In our calculations, the increasing ﬁltration isobtained by reversing the attention matrix weights:w7!1\u0000w.Thep-th persistent homology of Kis the pairof sets of vector spaces fHp(Kti)j0\u0014i\u0014lgandmapsffi;j:Hp(Kti)!Hp(Ktj)j1\u0014i<j\u0014lg,where the maps are induced by the inclusion mapsKti!Ktj.Each persistent homology class \u000b",
      "in this se-quence is “born” at some Ktiand “dies” at someKtjor never dies (Barannikov, 2021, 1994). Onecan visualize this as an interval [ti;tj]or[ti;+1[.The collection of all such intervals is called the bar-code of the ﬁltration. It is the most useful invariantof the ﬁltration. Note that the information about thepersistent homology classes is generally essentialto calculate the barcode, whereas the informationabout the Betti numbers only is insufﬁcient.Figure 5: The distribution shift of the sum of the bars in H0between the natural and generated texts for differentlayers and attention heads on the WebText & GPT-2 dataset. Top=(Layer: 7; Head: 5), (Layer: 8; Head: 10);Middle=(Layer: 9; Head: 11), (Layer: 9; Head: 7); Bottom=(Layer: 0; Head: 3), (Layer: 5; Head: 6). TGM:GPT-2 Small with pure sampling.In the case of the ﬁltration associated to aweighted graph (2), the H0\u0000th barcode (respec-tively,H1\u0000th) consists of the intervals of the form[0;di](resp., [si;1[) only. Given a number l, thenumber of intervals of length at most lforH0(re-spectively, the number of intervals with the leftendpoint at most l) is therefore equal to the theBetti number \f",
      "0(Kl)(resp.,\f",
      "1(Kl)). We see thatin this case, we can recover the barcode from thecollection of all Betti numbers \f",
      "i(Ktj)where thethresholds setftjgis the set of all weights of theedges in the graph. On the other hand, a calculationof theH0\u0000barcode gives all Betti numbers \f",
      "i(Ktj)simultaneously for all thresholds.In TDA, the following ﬁltered simplicial com-plex is also commonly associated with a graph.Then-dimensional simplices of the clique com-plex (or Whitney complex) X=X(G)of a graphG= (E;V)are the (n+ 1) -cliques ofG, that is,complete subgraphs with n+ 1vertices. For exam-ple, its 0-simplices are the vertices, the 1-simplicesare the edges, and the 2-simplices are the trian-gles. The clique complex has a richer topologicalstructure than the graph itself since it may havenonzero Betti numbers \f",
      "n(X)forn\u00152. Notethat theH0\u0000barcode of the ﬁltered clique complexcoincides with the H0\u0000barcode of (2). Severalsoftware packages are aimed at calculating the per-sistent homology of graph clique complex. Forthis purpose, we have used Ripser++ (Bauer, 2021;Zhang and Xiao, 2020).The entropy of the barcode is a measure of theentropy of the points in a persistence diagram. Pre-cisely, if we have a barcode as a list of pairs of“birth”, “death”: D=f(bi;di)gi2I, then the en-tropy is deﬁned as:E(D) =\u0000Xi2Ipilog(pi)wherepi=di\u0000biLD, andLD=Pi2I(di\u0000bi).Each bar in H0\u0000barcode has the form [0;di]where 1\u0000diis the attention weight of the edgewhich “kills” the connected component correspond-ing to this bar. The sum of lengths of bars in theH0\u0000barcode coincides with 128\u0000MwhereMis the sum of edge weights of the attention graphmaximal spanning tree. Examples of the shift ofdistributions of the sum of bars’ lengths betweennatural and generated texts are shown in the topand middle rows in Figure 5.C Training DetailsTopological Features and TF-IDF Similar to(Solaiman et al., 2019), the training of the lin-ear classiﬁers is run with the regularization pa-rameterL22[1e\u00005;5e\u00005;:::; 0:1;0:5;1]andthe maximum number of iterations max iter2[1;2;3;5;10;100] tuned on the validation set. Thetopological features are concatenated for eachattention head for each encoder layer, and fur-ther concatenated. The total number of fea-tures for each method is equal to 12\u000212\u0002number of features per method.BERT-based classiﬁers are trained with the lin-ear scheduler with the initial learning rate lr2[1e\u00005;5e\u00005;1e\u00004;:::; 1e\u00001;1]and epochs num-bere2[2;3;5;10;15;20].BERT [CLS trained]is trained for 20 epochs, and BERT [Fullytrained] is trained for 2-5 epochs depending onthe dataset. We use early stopping controlled bythe accuracy on the validation set for each text de-tection dataset.\n"
     ]
    },
    {
     "ename": "ServiceUnavailableError",
     "evalue": "The server is overloaded or not ready yet.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mServiceUnavailableError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(text)\n\u001b[0;32m---> 23\u001b[0m final_memory, steps, responses \u001b[38;5;241m=\u001b[39m \u001b[43mextract_info_from_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthegap\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m result_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_memory\u001b[39m\u001b[38;5;124m'\u001b[39m:final_memory, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteps\u001b[39m\u001b[38;5;124m'\u001b[39m:steps, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponses\u001b[39m\u001b[38;5;124m'\u001b[39m:responses}\n\u001b[1;32m     25\u001b[0m file_steps[file] \u001b[38;5;241m=\u001b[39m result_dict\n",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m, in \u001b[0;36mextract_info_from_text\u001b[0;34m(text, questions, thegap)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;241m/\u001b[39mthegap) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      6\u001b[0m     paper_chunk \u001b[38;5;241m=\u001b[39m text[i\u001b[38;5;241m*\u001b[39mthegap: (i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39mthegap]\n\u001b[0;32m----> 7\u001b[0m     response, extracted_info \u001b[38;5;241m=\u001b[39m \u001b[43mextract_info_chatgpt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaper_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     responses\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m'\u001b[39m:i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m:response, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunk\u001b[39m\u001b[38;5;124m'\u001b[39m:paper_chunk, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextracted_info\u001b[39m\u001b[38;5;124m'\u001b[39m:extracted_info})\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m, in \u001b[0;36mextract_info_chatgpt\u001b[0;34m(text, questions, prev_dict)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_info_chatgpt\u001b[39m(text, questions, prev_dict):\n\u001b[1;32m      2\u001b[0m     message \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      3\u001b[0m             {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a helpful assistant.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m      4\u001b[0m             {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe previous info dict is \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39mprev_dict},\n\u001b[1;32m      5\u001b[0m             {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI have full understanding of this chunk of article: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39mtext},\n\u001b[1;32m      6\u001b[0m             {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExtract these relevant information in English and organise it as \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPython dict\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (IMPORTANT), value set as \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn/a\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m if not applicable and append if multiple answers come out. Be particularly cautious on the n/a in the previous info dict and try your best to update precise info. Questions: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39mquestions }\n\u001b[1;32m      7\u001b[0m         ]\n\u001b[0;32m----> 8\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     extracted_info \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response, extracted_info\n",
      "File \u001b[0;32m~/sk_courses/env/lib/python3.8/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/sk_courses/env/lib/python3.8/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/sk_courses/env/lib/python3.8/site-packages/openai/api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    288\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[1;32m    289\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[1;32m    290\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    297\u001b[0m     )\n\u001b[0;32m--> 298\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/sk_courses/env/lib/python3.8/site-packages/openai/api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    693\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    694\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    695\u001b[0m         )\n\u001b[1;32m    696\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[1;32m    697\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 700\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    707\u001b[0m     )\n",
      "File \u001b[0;32m~/sk_courses/env/lib/python3.8/site-packages/openai/api_requestor.py:743\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m OpenAIResponse(\u001b[38;5;28;01mNone\u001b[39;00m, rheaders)\n\u001b[1;32m    742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rcode \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m503\u001b[39m:\n\u001b[0;32m--> 743\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mServiceUnavailableError(\n\u001b[1;32m    744\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe server is overloaded or not ready yet.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    745\u001b[0m         rbody,\n\u001b[1;32m    746\u001b[0m         rcode,\n\u001b[1;32m    747\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrheaders,\n\u001b[1;32m    748\u001b[0m     )\n\u001b[1;32m    749\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    750\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext/plain\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m rheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[0;31mServiceUnavailableError\u001b[0m: The server is overloaded or not ready yet."
     ]
    }
   ],
   "source": [
    "    pdf_path = '/home/prateek/sk_courses/IE_chatGPT/test_pdfs/'\n",
    "    files = [x for x in os.listdir(pdf_path)]\n",
    "\n",
    "    thegap = 8000\n",
    "    questions = ['What is the research object of this article',\n",
    "                 'What is (are) the country/region of the study',\n",
    "                 'What is the data sample size (or observations or n)',\n",
    "                 'What is the theory name of this paper',\n",
    "                 'What is the study period of this paper (year or year range)',\n",
    "                 'Is this paper a qualitative or quantitative study',\n",
    "                 'How many authors are there in this paper',\n",
    "                 'What are the main findings for this paper',\n",
    "                 'What is (are) the methodology name(s) of this paper (or the type of regression etc)',\n",
    "                 'Is this paper an original study or literature review',\n",
    "                 'Is this chunk looks like reference list? Answer this question with \"ENDENDEND\" if so else leave it blank as \"\"']\n",
    "\n",
    "    start_time = time.time()\n",
    "    file_steps = {}\n",
    "    for file in files:\n",
    "        text = read_pdf(pdf_path+file)\n",
    "        text = text.replace('\\n','').replace('  ',' ').replace('  ',' ').replace('  ',' ').replace('  ',' ').strip()\n",
    "        print(text)\n",
    "        final_memory, steps, responses = extract_info_from_text(text, questions, thegap)\n",
    "        result_dict = {'final_memory':final_memory, 'steps':steps, 'responses':responses}\n",
    "        file_steps[file] = result_dict\n",
    "        print(file, '\\t', f'cost {time.time() - start_time} seconds')\n",
    "    end_time = time.time()\n",
    "    print(f'Cost time {end_time - start_time} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
